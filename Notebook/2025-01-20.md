# RP | 2025-01-20

## Bayesian inference

### Computing

- At time step $k$, $p(X_{1:k} \mid X_0, u_{1:k}, Z_{1:k}) \rightarrow$ probability density of the state sequence, given the history of inputs and measurements

  - Knowing the initial state $X_0$, the history of inputs $u_{1:k}$, and the history of measurements $Z_{1:k}$, it is possible to completely know/ characterise $X_{1:k}$

- Marginalisation
  - $P(A) = \int P(A, B) dB$
  

### Recursive implementation for Markov chains

- Required: $p(X_{k} \mid X_0, u_{1:k}, Z_{1:k})$

- Allows for calculating in an online setting

- By marginalising, we can find this
  $$
  p(X_{k} \mid X_0, u_{1:k}, Z_{1:k}) = \int p(X_{1:k} \mid X_0, u_{1:k}, Z_{1:k}) \, dX_{1:k-1}
  $$
  
- Now, in general, we have
  $$
  p(X_{1:k} \mid X_0, u_{1:k}, Z_{1:k}) = p(X_k \mid X_{1:k-1}, X_0, u_{1:k}, Z_{1:k}) p(X_{1:k-1} \mid X_0, u_{1:k}, Z_{1:k})
  $$
  
- The first term can be simplified, by the Markov property, and by the Bayes rule, respectively, as
  $$
  p(X_k \mid X_{1:k-1}, X_0, u_{1:k}, Z_{1:k}) = p(X_k \mid X_{k-1}, u_{k}, Z_{k}) = \eta_k p(Z_k \mid X_k, X_{k-1}, u_{k}) p(X_k \mid X_{k-1}, u_{k})
  $$

- Now, since measurement only depends on the current state, we have
  $$
  p(Z_k \mid X_k, X_{k-1}, u_{k}) = p(Z_k \mid X_k)
  $$

- Gaussian yields Gaussian

- Update values need a probability model to be defined

- References
  - Probabilistic Robotics, Sebastian Thrun, 2005; Chapter-3; Derivation

### Recursive implementation via Extended Kalman filter

- Let $p(X_{k-1} \mid X_0, u_{1:k-1}, Z_{1:k-1}) = \mathcal{N}(\mu_{k-1}, \Sigma_{k-1})$

- Motion model

  - $$
    R_{k+1} = R_{k} \operatorname{Exp}(\omega_k T_s) + \text{Noise}
    \\
    d_{k+1} = d_k + R_k v_k T_s + \text{Noise}
    $$

#### Extended Kalman filter

- Setup
  $$
  X_t = g(X_{t-1}, u_t) + r_t, \quad r_t \sim \mathcal{N}(0, R_t)
  \\
  Z_t = h(X_{t}) + q_t, \quad q_t \sim \mathcal{N}(0, Q_t)
  \\
  G = \mathcal{J}_{\mu_{t-1}}(g), \qquad H = \mathcal{J}_{\mu_{t}}(h)
  $$

- Jacobian $\mathcal{J}$

- Change of mean and covariance
  $$
  Y = AX
  \\
  \Sigma_Y = A \Sigma_X A^\top
  $$

- EKF equations



#### Limitations

- Due to non-linearities, even mean and covariance don't match between expected Gaussian
  - Want to minimise the error and still use Gaussians

## Histogram

## Particle filters

- Propagates particle density from input to the output

## Information filter

- Mahalanobis distance

---

