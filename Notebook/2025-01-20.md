# RP | 2025-01-20

## Bayesian inference

### Computing

- At time step $k$, $p(X_{1:k} \mid X_0, u_{1:k}, Z_{1:k}) \rightarrow$ probability density of the state sequence, given the history of inputs and measurements

  - Knowing the initial state $X_0$, the history of inputs $u_{1:k}$, and the history of measurements $Z_{1:k}$, it is possible to completely know/ characterise $X_{1:k}$

- Marginalisation
  - $P(A) = \int P(A, B) dB$
  

### Recursive implementation for Markov chains

- Required: $p(X_{k} \mid X_0, u_{1:k}, Z_{1:k})$

- Allows for calculating in an online setting

- By marginalising, we can find this
  $$
  p(X_{k} \mid X_0, u_{1:k}, Z_{1:k}) = \int p(X_{1:k} \mid X_0, u_{1:k}, Z_{1:k}) \, dX_{1:k-1}
  $$

- Now, in general, we have
  $$
  p(X_{1:k} \mid X_0, u_{1:k}, Z_{1:k}) = p(X_k \mid X_{1:k-1}, X_0, u_{1:k}, Z_{1:k}) p(X_{1:k-1} \mid X_0, u_{1:k}, Z_{1:k})
  $$

- The first term can be simplified, by the Markov property, and by the Bayes rule, respectively, as

  $$
  p(X_k \mid X_{1:k-1}, X_0, u_{1:k}, Z_{1:k}) = p(X_k \mid X_{k-1}, u_{k}, Z_{k}) = \eta_k p(Z_k \mid X_k, X_{k-1}, u_{k}) p(X_k \mid X_{k-1}, u_{k})
  $$

- Now, since measurement only depends on the current state, we have

  $$
  p(Z_k \mid X_k, X_{k-1}, u_{k}) = p(Z_k \mid X_k)
  $$

  $$
  p(X_{k} \mid X_0, u_{1:k}, Z_{1:k}) = \int p(X_{k} \mid X_{k-1}, u_{k}, Z_{k}) \left( \int p(X_{1:k-1} \mid X_0, u_{1:k}, Z_{1:k}) \, dX_{1:k-2} \right) \, dX_{k-1}
  $$

  $$
  p(X_{k} \mid X_0, u_{1:k}, Z_{1:k}) = \underbrace{ \eta_k \ p(Z_k \mid X_k) }_{\text{update}} \underbrace{ \int p(X_{k} \mid X_{k-1}, u_{k}) \ p(X_{k-1} \mid X_0, u_{1:k-1}, Z_{1:k-1}) \, dX_{k-1} }_{\text{prediction}}
  $$

- Gaussian yields Gaussian

  - If all probabilities on the RHS are assumed to be Gaussian $\implies$ LHS is also Gaussian

- Can be solved exactly leading to the famous Kalman filter equations

- Update values need a probability model to be defined

- References
  - Probabilistic Robotics, Sebastian Thrun, 2005; Chapter-3; Derivation

### Recursive implementation via Extended Kalman filter

- Let $p(X_{k-1} \mid X_0, u_{1:k-1}, Z_{1:k-1}) = \mathcal{N}(\mu_{k-1}, \Sigma_{k-1})$

- Motion model
  $$
  R_{k+1} = R_{k} \operatorname{Exp}(\omega_k T_s) + \text{Noise}
  \\
  d_{k+1} = d_k + R_k v_k T_s + \text{Noise}
  $$
  
- Measurement model

  - $p(Z_k \mid X_k)$ depends on the sensor

  - Eg: Range finder
    $$
    Z_k = d_k + \mathcal{N}(0, \Sigma_m)
    $$

- Measuring time, with a clock

#### Extended Kalman filter

- Setup
  $$
  X_t = g(X_{t-1}, u_t) + r_t, \quad r_t \sim \mathcal{N}(0, R_t)
  \\
  Z_t = h(X_{t}) + q_t, \quad q_t \sim \mathcal{N}(0, Q_t)
  \\
  G = \mathcal{J}_{\mu_{t-1}}(g), \qquad H = \mathcal{J}_{\mu_{t}}(h)
  $$

- Jacobian $\mathcal{J}$

- Change of mean and covariance
  $$
  Y = AX
  \\
  \Sigma_Y = A \Sigma_X A^\top
  $$

- EKF equations

- Measurement noise and process noise

- Innovation $z_t - h(\bar\mu_t)$

  - Expected to see $h(\bar\mu_t)$, but saw $z_t$, thereby tried to account for


#### Limitations

- Due to non-linearities, even mean and covariance don't match between expected Gaussian
  - Want to minimise the error and still use Gaussians

## Histogram

- Histogram representation ~simplifies
- There might be a possibility to lose bins in the input histogram that may not appear in the output histogram, since the input bins might be very small

## Particle filters

- For every particle, essentially propagates particle density from input to the output
- Representing distributions
- ~The particles density represents the local probability density

## Information filter

- Instead of using the covariance matrix, uses the precision matrix/ information matrix
- Precision matrix is the inverse of the covariance matrix, $\Omega = \Sigma^{-1}$

$$
p(X) = C e^{-(X - \mu)^\top \Sigma^{-1} (X - \mu)} = C' e^{X^\top \Omega X + X^\top \Omega \mu}
$$

- $X^\top \Omega X \to$ Mahalanobis distance
- $\xi = \Sigma^{-1} \mu$

### Algorithm: Information filter

- $\operatorname{IF}(\xi_{t-1}, \Omega_{t-1}, u_t, z_t)$

$$
\begin{aligned}
\bar\Omega_t
& =
{(A_t \Omega_{t-1}^{-1} A_t^\top + R_t)}^{-1}
\\
\bar\xi_t
& =
\bar\Omega_t (A_t \Omega_{t-1}^{-1} \xi_t^\top + B_t u_t)
\end{aligned}
$$

$$
\begin{aligned}
\Omega_t
& =
C_t^\top Q_t^{-1} C_t + \bar\Omega_t
\\
\xi_t
& =
C_t^\top Q_t^{-1} z_t + \bar\xi_t
\end{aligned}
$$

---

