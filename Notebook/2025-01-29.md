# RP | 2025-01-29

## Review

- Motion-measurement model gets linearised
- Limitations
  - Non-linearities should be small
  - Variances should be small
  - ~ Else there is an offset in the amplitude of the Gaussians

## Unscented transform

- Sigma points

  - Since less than a standard deviation away


$$
  \begin{aligned}
  \mathcal{X}^{[0]} &= \mu \\
  \mathcal{X}^{[i]} &= \mu + \left(\sqrt{(n+\lambda) \Sigma} \right)_i, \quad \text{for } i = 1, \dots, n \\
  \mathcal{X}^{[i]} &= \mu - \left(\sqrt{(n+\lambda) \Sigma} \right)_{i-n}, \quad \text{for } i = n+1, \dots, 2n
  \end{aligned}
$$

- Weights

  $$
  w_m^{[i]} = w_c^{[i]} = \frac{1}{2(n+\lambda)}, \quad \text{for } i = 1, \dots, 2n.
  $$

- Non-linear mapping

  $$
  \mathcal{Y}^{[i]} = g(\mathcal{X}^{[i]})
  $$

- Moments for mapped Gaussian

  $$
  \begin{aligned}
  \mu' &= \sum_{i=0}^{2n} w_m^{[i]} \mathcal{Y}^{[i]} \\
  \Sigma' &= \sum_{i=0}^{2n} w_c^{[i]} (\mathcal{Y}^{[i]} - \mu') (\mathcal{Y}^{[i]} - \mu')^T.
  \end{aligned}
  $$
  
- Comparision with EKF linearisation
  - UKF is better in terms of matching to real distribution's moments

- Resources
  - <https://en.wikipedia.org/wiki/Unscented_transform>
  - <https://en.wikipedia.org/wiki/Kalman_filter#Unscented_Kalman_filter>
  - <https://gregorygundersen.com/blog/2020/11/19/unscented-transform/>
  - <https://www.mitre.org/sites/default/files/pdf/vanzandt_unscented.pdf>
  - <https://ethw.org/First-Hand:The_Unscented_Transform>
  - <https://www.eecs.yorku.ca/course_archive/2017-18/W/4421/lectures/Unscented%20kalman%20filter%20-%20annotated.pdf>

### Algorithm: Unscented Kalman filter

- $\operatorname{UKF} (\mu_{t-1}, \Sigma_{t-1}, u_t, z_t)$

- Prediction step
  $$
  \begin{aligned}
      \mathcal{X}_{t-1} &= \left( \mu_{t-1}, \quad \mu_{t-1} + \gamma \sqrt{\Sigma_{t-1}}, \quad \mu_{t-1} - \gamma \sqrt{\Sigma_{t-1}} \right) \\
      \mathcal{X}_t^* &= g(u_t, \mathcal{X}_{t-1}) \\
      \bar{\mu}_t &= \sum_{i=0}^{2n} w_m^{[i]} \mathcal{X}_t^{*[i]} \\
      \bar{\Sigma}_t &= \sum_{i=0}^{2n} w_c^{[i]} (\mathcal{X}_t^{*[i]} - \bar{\mu}_t)(\mathcal{X}_t^{*[i]} - \bar{\mu}_t)^T + R_t
  \end{aligned}
  $$

- Update step
  $$
  \begin{aligned}
      \mathcal{X}_t &= \left( \bar{\mu}_t, \quad \bar{\mu}_t + \gamma \sqrt{\bar{\Sigma}_t}, \quad \bar{\mu}_t - \gamma \sqrt{\bar{\Sigma}_t} \right) \\
      \mathcal{Z}_t &= h(\mathcal{X}_t) \\
      \hat{z}_t &= \sum_{i=0}^{2n} w_m^{[i]} \mathcal{Z}_t^{[i]} \\
      S_t &= \sum_{i=0}^{2n} w_c^{[i]} (\mathcal{Z}_t^{[i]} - \hat{z}_t)(\mathcal{Z}_t^{[i]} - \hat{z}_t)^T + Q_t \\
      \bar{\Sigma}_t^{x,z} &= \sum_{i=0}^{2n} w_c^{[i]} (\mathcal{X}_t^{[i]} - \bar{\mu}_t)(\mathcal{Z}_t^{[i]} - \hat{z}_t)^T \\
      K_t &= \bar{\Sigma}_t^{x,z} S_t^{-1} \\
      \mu_t &= \bar{\mu}_t + K_t (z_t - \hat{z}_t) \\
      \Sigma_t &= \bar{\Sigma}_t - K_t S_t K_t^T
  \end{aligned}
  $$

- A derivative free filter, no Jacobians needed

- Implicitly doing a numerical filter

- Useful for functions which don't have an analytical form

- Process noise $R_t$ used for predicting, sensor noise $Q_t$ used for correcting

## Particle filters

- Resources
  - <https://en.wikipedia.org/wiki/Particle_filter>
  - <https://en.wikipedia.org/wiki/Monte_Carlo_localization>
  - <https://en.wikipedia.org/wiki/Filtering_problem_(stochastic_processes)>

- Distribution is maintained as a collection of sampled particles
  - Distribution is representation as a set/ collection of particles

- $M \to$ Number of particles; Usually large enough to conver the state space adequately, and is a hyper parameter

- $p(y_k) \Delta y \sim$ fraction of particles in the interval $\{ y_k - \frac{\Delta y}{2}, y_k + \frac{\Delta y}{2} \}$

- Histogram
  - $h = \{ h_0, h_1, \dots, h_{m-1} \}$
  - Support

- Belief $\operatorname{bel}(\cdot)$

- Recursive process

  - With marginalisation

    $$
    \operatorname{bel} (X_k) \triangleq p(X_{k} \mid X_0, u_{1:k}, Z_{1:k})
    $$

    $$
    \implies \operatorname{bel} (X_k) = \eta_k \ p(Z_k \mid X_k) \int p(X_{k} \mid X_{k-1}, u_{k}) \operatorname{bel} (X_{k-1}) \, dX_{k-1}
    $$
    
  - Without marginalisation

    $$
    \operatorname{bel}_{k:1} \triangleq p(X_{k:1} \mid X_0, u_{1:k}, Z_{1:k})
    $$

    $$
    \implies \operatorname{bel}_{k:1} = \eta_k \ p(Z_k \mid X_k) \int p(X_{k} \mid X_{k-1}, u_{k}) \operatorname{bel}_{k-1:1} \, dX_{k-1}
    $$

- Likelihood $p(Z_k \mid X_k)$

- Loop over each particle, $i = 1, 2, \dots, M$
  $$
  \bar X_k^i = P(X \mid \bar X_{k-1}^i, U_k) \quad \sim \mathcal{N}(g(X_{k-1}^i, U_k), R_t)
  $$
  where $R_t$ is the process covariance
  $$
  \implies e^{-{(X - g(X_{k-1}^i, U_k))}^\top R_t^{-1} (X - g(X_{k-1}^i, U_k))}
  $$
  $W_k^i = P(Z_k \mid \bar X_k^i)$ and take $<W_k^i, \bar X_k^i>$

- References: Thrun, Chapter 4, 8

### Importance sampling

- Resources

  - <https://en.wikipedia.org/wiki/Importance_sampling>

- A variance reduction technique
  $$
  \operatorname{E}[B(X)] = \int_X B(X) f(X) \, dX \approx \frac{1}{M} \sum_{X_i \sim f(X)} B(X_i)
  $$

  $$
  f(X) = \left\{ \eta_k \ p(Z_k \mid X_k) \ p(X_{k} \mid X_{k-1}, u_{k}) \operatorname{bel} (X_{k-1}) \right\}
  $$

  $$
  \operatorname{E}[B(X)] = \int B(x) g(x) \underbrace{ \frac{f(x)}{g(x)} }_{\text{weight}} \, dx
  $$

- Proposal distribution $\sim \cfrac{f(x)}{g(x)}$
  $$
  \int B(X_k) \operatorname{bel}(X_k) \, d X_k
  \implies
  \int B(X_k) \ p(X_k \mid X_{k-1}) \operatorname{bel}(X_{k-1}) \frac{\operatorname{bel}(X_k)}{p(X_k \mid X_{k-1}) \operatorname{bel}(X_{k-1})} \, d X_k
  \\
  \frac{\operatorname{bel}(X_k)}{p(X_k \mid X_{k-1}) \operatorname{bel}(X_{k-1})} = p(Z_k \mid X_k)
  $$
  
  $$
  <p(Z_k \mid \bar X_k^i), \bar X_k^i>, \quad i = 1, 2, \dots, N
  $$

---

