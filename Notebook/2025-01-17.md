# RP | 2025-01-17

## Probabilistic state estimation

- Bring in other sensor modalities, Eg: LIDAR, etc

## State

- Modelled as a random vector, due to the uncertainty involved
- A multi-dimensional vector, $X_t \in \mathbb{R}^n$
  - Pose of robot, $X_t = (R_t, d_t) \in \mathbb{R}^6$
  - State of charge of battery, $Q \in \mathbb{R}$
  - State of the world
- The challenge of estimation is the implementation part
- What to include is dependent on what is intended to control or observe
- Characterised by a probability density function $p(X_t)$, to handle the uncertainty
  - Typically, captured through the first two moments 
  - Mean, $\mu_t = \operatorname{E}[X_t] \in \mathbb{R}^n$
  - Covariance, $\Sigma_t = \operatorname{E}[(X_t - \mu_t)(X_t - \mu_t)^\top] \in \mathbb{R^{n \times n}}$
  - Modelling and approximation is very crucial, affects everything moving forward
  - Gaussing approximations
    - Gaussians can be fully specified with just mean and covariance
  - Random variables

### Complete state

- Captures the influence of all past actions, so that prediction of the future, given the state, doesn't depend on the past
- To satisfy the Markovian property

- Continuous, discrete or hybrid
  - Time is usually modelled discretely
  - Floating point considerations
    - IEEE 754?, or B4 format?

### State evolution

- Dynamics or kinematics govern state evolution
  - Model-based
    - Differential equations
    - Dynamical equations for state evolution
    - System identification might be required
      - Parameters of the equation may need to be identified
      - Part of separate calibration phase or state estimation
  - Model-free
    - No explicit model
      - In the sense, 
    - Learning from data
    - Learning state transitions (Implicit model)
    - https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)
  - Hybrid
    - Physics informed neural networks (PINN)

- $$
  X_{t+1} = f(X_t, u_t) + P_t
  $$

  - $u_t$ is the input to the system
  - $P_t$ is the disturbance

- Knowledge and lack of knowledge is dealt with how the designer models the situation
  - Eg: Detailed modelling of flipping a coin, vs modelling as a random variable
    - https://www.iflscience.com/scientists-win-ig-nobel-prize-for-proving-coin-tosses-are-not-5050-76061

- Gaussian assumption is usually justified by the Central limit theorem, in many cases
  - Also convenient to work with
  - Theoretically too
  - Depends on the situation, modelling choices

### State equations

- IMU measurements, agnostic to robot type

- State evolution equations

  - Without noise

  - $$
    R_{k+1} = R_k \operatorname{Exp}(\omega_k T_s)
    \\
    d_{k+1} = d_k + R_k v_k T_s
    $$

- Measurements $Z_{k+1}$ from LIDAR, Camera, GPS, etc


## Bayesian inference

- **Notation**: $X_{i:j} = X_{i}, X_{i+1}, \ldots, X_{j}$ represents a sequnce of variables

- At time step $k$, $p(X_{1:k} \mid X_0, u_{1:k}, Z_{1:k}) \rightarrow$ probability density of the state sequence, given the inputs and measurements

- From Bayes theorem, we have
  $$
  p(X_{1:k} \mid X_0, u_{1:k}, Z_{1:k}) = \frac{p(Z_{1:k} \mid X_{1:k}, X_0, u_{1:k}) p(X_{1:k} \mid X_0, u_{1:k})}{p(Z_{1:k} \mid X_0, u_{1:k})}
  $$

- Now, in general, we have
  $$
  p(X_{1:k} \mid X_0, u_{1:k}) = p(X_k \mid X_{1:k-1}, X_0, u_{1:k}) p(X_{1:k-1} \mid X_0, u_{1:k})
  $$

- By the Markovian assumption, we have
  $$
  p(X_k \mid X_{1:k-1}, u_{1:k}) = p(X_k \mid X_{k-1}, u_{k})
  $$

  $$
  p(X_{1:k} \mid X_0, u_{1:k}) = \prod_{i=1}^{k} p(X_i \mid X_{i-1}, u_i)
  $$

  $$
  p(Z_{1:k} \mid X_{1:k}, X_0, u_{1:k}) = \prod_{i=1}^{k} p(Z_i \mid X_{i})
  $$

- Allows for breaking down recursively to solve simpler versions of the problem; Following a **recursion** pattern

### Dynamic Bayesian network

- Every timestep, new nodes are added $\implies$ Dynamic

- ```mermaid
  graph LR
      subgraph "Time t-1"
          Xt_1[Xt-1]
          Ut_1[Ut-1]
          Zt_1[Zt-1]
          Ut_1 --> Xt_1
          Xt_1 --> Zt_1
      end
  
      subgraph "Time t"
          Xt[Xt]
          Ut[Ut]
          Zt[Zt]
          Xt --> Ut
          Xt --> Zt
          Xt_1 --> Xt
          Ut --> Xt
      end
  
      subgraph "Time t+1"
          Xt_1p[Xt+1]
          Ut_1p[Ut+1]
          Zt_1p[Zt+1]
          Xt_1p --> Ut_1p
          Xt_1p --> Zt_1p
          Xt --> Xt_1p
          Ut --> Xt_1p
      end
  
  ```

### Computing

- $$
  p(X_{1:k} \mid X_{0}, u_{1:k}, Z_{1:k}) = \eta \prod_{i=1}^{k} p(Z_{i} \mid X_{i}) p(X_{i} \mid X_{i-1}, u_{i}) = \eta p(Z_k \mid X_{k}) p(X_k \mid X_{k-1}, u_{k}) p(Z_{1:k-1} \mid X_0, u_{1:k-1}, Z_{1:k-1})
  $$

- $\eta \rightarrow$ ~normalising factor, Avoiding computation and utilising the proportionalities

- $$
  \operatorname{bel}_k \triangleq p(X_{1:k} \mid X_0, u_{1:k}, Z_{1:k})
  $$

- Calculate $p(X_k \mid X_{k-1}, u_{k})$, hence $p(X_{1:k} \mid X_0, u_{1:k}, Z_{1:k})$

- Above can be an arbitrary probability density function

- Marginalisation

- Alternatives: Histogram, Particle filters, Mixture of Gaussian

---

- References
  - Probabilistic Robotics, Sebastian Thrun, 2005; Chapter-2,3,4,*;

---

