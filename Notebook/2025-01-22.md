# RP | 2025-01-22

## EKF for a 3D robot

- Going from $k \to (k+1)$

- Pose

  $$
  T_k = \begin{bmatrix} R_k & d_k \\ 0_{1 \times 3} & 1 \end{bmatrix}_{4 \times 4} = \operatorname{Exp} \left( \begin{bmatrix} \theta_k \\ \rho_k \end{bmatrix} \right)
  $$

  $$
  R_k = e^{[\theta_k]_\times}
  , \qquad
  R_{k+1} = R_k e^{[\omega \Delta t]_\times}
  $$

$$
R_{k+1} = R_k e^{[\Delta \theta_k]_\times} \implies e^{[\theta_{k+1}]_\times} = e^{[\theta_k]_\times} e^{[\Delta \theta_k]_\times} = e^{[\theta_k + J_r^{-1} \Delta \theta_k]_\times}
$$

- $\theta_{k+1} = \theta_k + \Delta \theta_k \to$ valid in the 2D case, not in the 3D case

$$
\begin{aligned}
\theta_{k+1}
& =
\theta_k + \omega_{k+1} T_s + \mathcal{N}(0, R_{1k})
\\
d_{k+1}
& =
d_k + R(\theta_k) v_k T_s + \mathcal{N}(0, R_{2k})
\end{aligned}
$$

$$
G_k = \begin{bmatrix} J \theta_k \\ J d_k \end{bmatrix}
= \begin{bmatrix}
\begin{bmatrix} I_3 & 0_{3 \times 3} \end{bmatrix}_{3 \times 6}
\\
\begin{bmatrix} R(\theta_k) [v_k T_s]_\times J_r^{-1} & I_3\end{bmatrix}_{3 \times 6}
\end{bmatrix}_{6 \times 6}
$$

### Algorithm: Extended Kalman filter

- Going from $(t-1) \to t$

- $\operatorname{EKF} (\mu_{t-1}, \Sigma_{t-1}, u_t, z_t)$

  - Prediction, $p(X_k \mid X_{k-1}, u_k)$

    $$
    \begin{aligned}
    \bar\mu_t
    & =
    g(u_t, \mu_{t-1})
    \\
    \bar\Sigma_t
    & =
    G_t \Sigma_{t-1} G_T^\top + R_t
    \end{aligned}
    $$

  - Kalman gain

    $$
    K_t = \bar\Sigma_t H_t^\top {\left( H_t \bar\Sigma_t H_t^\top + Q_t \right)}^{-1}
    $$

  - Update, $p(Z_k \mid X_k) \ p(X_k \mid X_{k-1}, u_k)$

    $$
    \begin{aligned}
    \mu_t
    & =
    \bar\mu_t + K_t (z_t - h(\bar\mu_t))
    \\
    \Sigma_t
    & =
    (I - K_t H_t) \bar\Sigma_t
    \end{aligned}
    $$

## EKF SLAM

- SLAM: Simultaneous localisation and mapping
- A map is a collection of features in the environment and its locations
- A feature is an abstract concept, that can be identified and recognised
- Pose; location, orientation; Identification; shape, semantics; Static features in the global world frame
- Point features, line features, etc
- A map is a set/ collection of features, $m = \{ l_1, l_2, \dots, l_j \}$
- Every feature has an ID and a location
  - 2D location $\implies \begin{bmatrix} l_{nx} \\ l_{ny} \end{bmatrix}, n = 1, 2, \dots j$
- The measurements $Z_t$ not only depends on the state $X_t$ but also on the map $m$
- Need to determine: $p(x_t, m \mid z_{1:t}, u_{1, t})$
- Modelled as a random vector; ~ the probability distribution also ges updated based on the path, etc
- As you move, might want to add some features, some features might get deleted
- Data association
- LiDAR sensor
  - Performs a scan by shooting a LASER beam, measures time difference of reflection, giving distance
  - ~ Orientation while shooting is known
  - Shooting from different positions, to achieve a scan

- Measurement model
  $$
  Z_{kn} = \begin{bmatrix} d_{kn} \\ \alpha_{kn} \end{bmatrix}
  $$

  - $\alpha_{kn}$ is w.r.t. to the body frame since the LiDAR is mounted on the body

  $$
  X_{k} = \begin{bmatrix} \mu_{kn} \\ \mu_{ky} \end{bmatrix}
  $$

  $$
  \begin{aligned}
  d_{kn}
  & =
  \sqrt{(l_{nx} - \mu_{kx})^2 + (l_{ny} - \mu_{ky})^2}
  \\
  \alpha_{kn}
  & =
  \tan^{-1} \frac{l_{ny} - X_{ky}}{l_{nx} - X_{kx}} - \theta_k
  \end{aligned}
  $$

- References
  - Thrun, "Probabilistic robotics"
  - Cyrill Stachniss, Photogrammetry & Robotics lab

---

- Estimate robot's pose and locations of the landmarks in the environment
- Assumption: Known correspondances

- State space for the 2D plane

  $$
  x_t = ( \underbrace{x, y, \theta}_{\text{robot's pose}}, \underbrace{m_{1,x}, m_{1,y}}_{\text{landmark 1}}, \dots, \underbrace{m_{n,x}, m_{n,y}}_{\text{landmark n}} )^\top
  $$

  - A map with $n$ landmarks $\implies (2n + 3)$ dimensional Gaussian

- State representation

  $$
  \underbrace{
  \begin{bmatrix}
  x_t \\
  m_1 \\
  \vdots \\
  m_n
  \end{bmatrix}
  }_{\mu}
  =
  \underbrace{
  \begin{bmatrix}
  \Sigma_{x_t x_t} & \Sigma_{x_t m_1} & \dots & \Sigma_{x_t m_n} \\
  \Sigma_{m_1 x_t} & \Sigma_{m_1 m_1} & \dots & \Sigma_{m_1 m_n} \\
  \vdots & \vdots & \ddots & \vdots \\
  \Sigma_{m_n x_t} & \Sigma_{m_n m_1} & \dots & \Sigma_{m_n m_n}
  \end{bmatrix}
  }_{\Sigma}
  $$

  $$
  \underbrace{
  \begin{bmatrix}
  x \\
  m
  \end{bmatrix}
  }_{\mu}
  =
  \underbrace{
  \begin{bmatrix}
  \Sigma_{x x} & \Sigma_{x m} \\
  \Sigma_{m x} & \Sigma_{m m}
  \end{bmatrix}
  }_{\Sigma}
  $$

  - Covariances and cross-covariances

- Filter cycle
  - State prediction
  - Measurement prediction
  - Measurement
    - Measurement of landmarks
    - Find Kalman gain
  - Data association
    - Need to know which measurement corresponds to which landmark
  - Update
    - Update the states

- Motion model for robot
- Motion model for the map variables
  - Assumption is that the map features are fixed (~over time)
- Camera
  - Can obtain angles
  - With stereo cameras, can also get the distance
- Challenge/Trick is to do this fast and real-time, since complexity can increase with more landmarks over time

---

